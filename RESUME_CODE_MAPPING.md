# ç®€å†å†…å®¹ä¸ä»£ç å¯¹ç…§æ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ç®€å†ä¸­çš„æ¯ä¸€é¡¹æŠ€æœ¯ç‚¹åœ¨é¡¹ç›®ä»£ç ä¸­çš„å…·ä½“ä½ç½®ï¼Œæ–¹ä¾¿é¢è¯•å‡†å¤‡å’ŒæŠ€æœ¯å¤ç›˜ã€‚

---

## ğŸ“ ç®€å†æè¿°ï¼ˆå®Œæ•´ç‰ˆï¼‰

```
ç”¨äºå‘½åå®ä½“è¯†åˆ«çš„æ·±åº¦å­¦ä¹ æ¶æ„æ¯”è¾ƒç ”ç©¶

â€¢ æ¨¡å‹å®ç°ï¼šå¯¹æ¯” BiLSTMï¼ˆåŒå‘ LSTM + 256ç»´éšè—å±‚ï¼‰å’Œ Transformerï¼ˆ8å¤´è‡ªæ³¨æ„åŠ› + 2å±‚ç¼–ç å™¨ï¼‰ä¸¤ç§æ¶æ„
  è‡ªå®šä¹‰å®ç°æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç æ¨¡å—ï¼Œç†è§£ Transformer ä½ç½®ä¿¡æ¯ç¼–ç çš„æ•°å­¦åŸç†
  åº”ç”¨ Dropout æ­£åˆ™åŒ–ï¼ˆ0.3ï¼‰å’Œ Embedding å±‚ï¼ˆ128ç»´ï¼‰ä¼˜åŒ–æ¨¡å‹æ€§èƒ½

â€¢ æ•°æ®å·¥ç¨‹ï¼šå¤„ç†çº¦ 1,700 ä¸ªæ ‡æ³¨å¥å­ï¼Œæ„å»ºç«¯åˆ°ç«¯ NLP é¢„å¤„ç†æµç¨‹
  å®ç°åˆ†è¯ã€è¯è¡¨æ„å»ºï¼ˆæ”¯æŒ <PAD>/<UNK> ç‰¹æ®Šæ ‡è®°å’Œè¯é¢‘è¿‡æ»¤ï¼‰ã€åºåˆ—å¡«å……ç®—æ³•
  è®¾è®¡è‡ªå®šä¹‰ Dataset å’Œ DataLoaderï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ï¼ˆbatch_size=32ï¼‰å’Œ IOB æ ‡æ³¨æ ¼å¼

â€¢ è®­ç»ƒä¼˜åŒ–ï¼š5-Fold äº¤å‰éªŒè¯ + Adam ä¼˜åŒ–å™¨ï¼ˆlr=1e-3ï¼‰+ äº¤å‰ç†µæŸå¤±å‡½æ•°
  è®­ç»ƒ 30 epochï¼Œå®æ—¶è¿½è¸ª 4 é¡¹æŒ‡æ ‡ï¼ˆè®­ç»ƒ/éªŒè¯çš„æŸå¤±å’Œå‡†ç¡®ç‡ï¼‰
  è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ›²çº¿å¯è§†åŒ–ï¼Œç›‘æ§æ¨¡å‹æ”¶æ•›çŠ¶æ€å’Œè¿‡æ‹Ÿåˆé£é™©

â€¢ è¯„ä¼°åˆ†æï¼šå¤šç»´åº¦è¯„ä¼°ä½“ç³» - ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1-Scoreï¼ˆMacroï¼‰å’Œæ··æ·†çŸ©é˜µ
  é’ˆå¯¹æ¯ä¸ªå®ä½“ç±»åˆ«ï¼ˆPERã€LOCã€ORGï¼‰å•ç‹¬åˆ†ææ€§èƒ½è¡¨ç°
  ç”Ÿæˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾ï¼Œè¯†åˆ«æ¨¡å‹é¢„æµ‹çš„å¸¸è§é”™è¯¯æ¨¡å¼
  Transformer æ¨¡å‹ F1 çº¦ 42%ï¼ˆå°è§„æ¨¡æ•°æ®é›†åœºæ™¯ï¼‰

â€¢ å·¥ç¨‹å®è·µï¼šæ¨¡å—åŒ–è®¾è®¡ï¼ˆæ•°æ®/æ¨¡å‹/è®­ç»ƒ/è¯„ä¼°åˆ†ç¦»ï¼‰+ CLI æ¥å£ + å•å…ƒæµ‹è¯•
  ç¼–å†™è¯¦ç»†æŠ€æœ¯æ–‡æ¡£ï¼ŒåŒ…æ‹¬é¡¹ç›®ç»“æ„ã€æŠ€æœ¯æ ˆè¯´æ˜ã€å¿«é€Ÿå¼€å§‹æŒ‡å—
  æä¾›çµæ´»çš„å‚æ•°é…ç½®ï¼Œæ”¯æŒå¿«é€Ÿæ¨¡å‹å¯¹æ¯”å®éªŒå’Œè¶…å‚æ•°è°ƒä¼˜

æŠ€æœ¯æ ˆï¼šPyTorch, NumPy, Scikit-learn, Matplotlib, Seaborn, Pytest
```

---

## ğŸ—ºï¸ ä»£ç ä½ç½®å¯¹ç…§è¡¨

### 1ï¸âƒ£ æ¨¡å‹å®ç°

#### âœ… BiLSTMï¼ˆåŒå‘ LSTM + 256ç»´éšè—å±‚ï¼‰
**æ–‡ä»¶**: `src/ner/models.py`  
**ä»£ç è¡Œ**: ç¬¬ 6-13 è¡Œ

```python
class BiLSTMTagger(nn.Module):
    def __init__(self, vocab_size: int, tagset_size: int, embedding_dim: int = 128, hidden_dim: int = 256):
        super(BiLSTMTagger, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(hidden_dim, tagset_size)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… `hidden_dim: int = 256` - 256ç»´éšè—å±‚
- âœ… `bidirectional=True` - åŒå‘LSTM
- âœ… `hidden_dim // 2` - å› ä¸ºåŒå‘LSTMï¼Œæ¯ä¸ªæ–¹å‘128ç»´ï¼Œåˆå¹¶å256ç»´

---

#### âœ… Transformerï¼ˆ8å¤´è‡ªæ³¨æ„åŠ› + 2å±‚ç¼–ç å™¨ï¼‰
**æ–‡ä»¶**: `src/ner/models.py`  
**ä»£ç è¡Œ**: ç¬¬ 38-46 è¡Œ

```python
class TransformerTagger(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, nhead=8, num_layers=2, max_len=100):
        super(TransformerTagger, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.pos_encoder = PositionalEncoding(embedding_dim, max_len=max_len)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(embedding_dim, tagset_size)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… `nhead=8` - 8ä¸ªæ³¨æ„åŠ›å¤´
- âœ… `num_layers=2` - 2å±‚Transformerç¼–ç å™¨
- âœ… `nn.TransformerEncoderLayer` - PyTorchå†…ç½®çš„Transformerå±‚

---

#### âœ… è‡ªå®šä¹‰å®ç°æ­£å¼¦/ä½™å¼¦ä½ç½®ç¼–ç æ¨¡å—
**æ–‡ä»¶**: `src/ner/models.py`  
**ä»£ç è¡Œ**: ç¬¬ 22-35 è¡Œ

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=100):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ç»´åº¦ä½¿ç”¨sin
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ç»´åº¦ä½¿ç”¨cos
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… **å®Œå…¨è‡ªå®šä¹‰å®ç°**ï¼ˆä¸æ˜¯PyTorchå†…ç½®ï¼‰
- âœ… æ•°å­¦å…¬å¼ï¼šPE(pos, 2i) = sin(pos / 10000^(2i/d_model))
- âœ… æ•°å­¦å…¬å¼ï¼šPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

**é¢è¯•è¯æœ¯**ï¼š
> "æˆ‘æ ¹æ® Transformer åŸè®ºæ–‡ï¼ˆAttention is All You Needï¼‰å®ç°äº†ä½ç½®ç¼–ç ã€‚ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°å¯ä»¥è®©æ¨¡å‹å­¦ä¹ ç›¸å¯¹ä½ç½®å…³ç³»ï¼Œå¹¶ä¸”å¯¹ä»»æ„é•¿åº¦çš„åºåˆ—éƒ½æœ‰æ•ˆã€‚"

---

#### âœ… Dropout æ­£åˆ™åŒ–ï¼ˆ0.3ï¼‰
**æ–‡ä»¶**: `src/ner/models.py`  
**ä»£ç è¡Œ**: ç¬¬ 12 è¡Œ

```python
self.dropout = nn.Dropout(0.3)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… Dropoutç‡ = 0.3ï¼ˆå³è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒ30%çš„ç¥ç»å…ƒï¼‰
- âœ… é˜²æ­¢è¿‡æ‹Ÿåˆ

---

#### âœ… Embedding å±‚ï¼ˆ128ç»´ï¼‰
**æ–‡ä»¶**: `src/ner/models.py`  
**ä»£ç è¡Œ**: ç¬¬ 7 è¡Œï¼ˆBiLSTMï¼‰ã€ç¬¬ 41 è¡Œï¼ˆTransformerï¼‰

```python
embedding_dim: int = 128
self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… å°†è¯è½¬æ¢ä¸º128ç»´å‘é‡
- âœ… `padding_idx=0` - å¡«å……ä½ç½®çš„åµŒå…¥ä¸º0å‘é‡

---

### 2ï¸âƒ£ æ•°æ®å·¥ç¨‹

#### âœ… å¤„ç†çº¦ 1,700 ä¸ªæ ‡æ³¨å¥å­
**éªŒè¯æ–¹æ³•**ï¼š
```python
# åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
from src.ner.data import read_data
sentences, tags = read_data('ner_dataset.txt')
print(f"Total sentences: {len(sentences)}")
```

**å®é™…æ•°é‡**ï¼šéœ€è¦è¿è¡Œä¸Šè¿°ä»£ç ç¡®è®¤

---

#### âœ… å®ç°åˆ†è¯
**æ–‡ä»¶**: `src/ner/data.py`  
**ä»£ç è¡Œ**: ç¬¬ 7-42 è¡Œ

```python
def read_data(file_path: str) -> Tuple[List[List[str]], List[List[str]]]:
    """è¯»å–æŒ‰è¡Œæ ‡æ³¨çš„NERæ•°æ®é›†ï¼Œè¿”å›(å¥å­åˆ—è¡¨, æ ‡ç­¾åˆ—è¡¨)"""
    # ...
    for line in f:
        line = line.strip()
        if not line:  # ç©ºè¡Œåˆ†éš”å¥å­
            # ...
        parts = line.split()  # åˆ†è¯ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²
        word, tag = parts[0], parts[-1]  # æå–è¯å’Œæ ‡ç­¾
        sentence.append(word)
        tag_seq.append(tag)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… æŒ‰è¡Œè¯»å–ï¼Œæ¯è¡Œä¸€ä¸ªè¯å’Œå¯¹åº”æ ‡ç­¾
- âœ… ç©ºè¡Œåˆ†éš”ä¸åŒå¥å­
- âœ… æ ¼å¼ï¼š`John B-PER`

---

#### âœ… è¯è¡¨æ„å»ºï¼ˆæ”¯æŒ <PAD>/<UNK> ç‰¹æ®Šæ ‡è®°å’Œè¯é¢‘è¿‡æ»¤ï¼‰
**æ–‡ä»¶**: `src/ner/data.py`  
**ä»£ç è¡Œ**: ç¬¬ 45-54 è¡Œ

```python
def build_vocab(sequences: List[List[str]], min_freq: int = 1) -> Dict[str, int]:
    word_freq = defaultdict(int)
    for seq in sequences:
        for token in seq:
            word_freq[token] += 1
    
    vocab = {"<PAD>": 0, "<UNK>": 1}  # ç‰¹æ®Šæ ‡è®°
    for word, freq in word_freq.items():
        if freq >= min_freq and word not in vocab:  # è¯é¢‘è¿‡æ»¤
            vocab[word] = len(vocab)
    return vocab
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… `<PAD>` (ç´¢å¼•0) - å¡«å……æ ‡è®°
- âœ… `<UNK>` (ç´¢å¼•1) - æœªçŸ¥è¯æ ‡è®°
- âœ… `min_freq` - ä½é¢‘è¯è¿‡æ»¤å‚æ•°

---

#### âœ… åºåˆ—å¡«å……ç®—æ³•
**æ–‡ä»¶**: `src/ner/data.py`  
**ä»£ç è¡Œ**: ç¬¬ 78-86 è¡Œ

```python
def __getitem__(self, idx: int):
    # ...
    length = len(word_ids)
    if length < self.max_len:
        # å¡«å……ï¼ˆPaddingï¼‰
        word_ids += [self.word2idx["<PAD>"]] * (self.max_len - length)
        tag_ids += [0] * (self.max_len - length)
    else:
        # æˆªæ–­ï¼ˆTruncationï¼‰
        word_ids = word_ids[:self.max_len]
        tag_ids = tag_ids[:self.max_len]
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… çŸ­åºåˆ—ï¼šç”¨ `<PAD>` å¡«å……åˆ° `max_len`
- âœ… é•¿åºåˆ—ï¼šæˆªæ–­åˆ° `max_len`
- âœ… `max_len=100` - æœ€å¤§åºåˆ—é•¿åº¦

---

#### âœ… è‡ªå®šä¹‰ Dataset å’Œ DataLoader
**æ–‡ä»¶**: `src/ner/data.py`  
**ä»£ç è¡Œ**: ç¬¬ 64-88 è¡Œï¼ˆDatasetï¼‰ã€ç¬¬ 97-100 è¡Œï¼ˆDataLoaderï¼‰

```python
class NERDataset(Dataset):
    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=100):
        # ...
    
    def __len__(self):
        return len(self.sentences)
    
    def __getitem__(self, idx):
        # è¿”å›ï¼š(word_ids, tag_ids, length)
        return torch.tensor(word_ids), torch.tensor(tag_ids), torch.tensor(length)

# DataLoaderä½¿ç”¨
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… ç»§æ‰¿ PyTorch çš„ `Dataset` ç±»
- âœ… å®ç° `__len__` å’Œ `__getitem__` æ–¹æ³•
- âœ… `DataLoader` è‡ªåŠ¨æ‰¹å¤„ç†å’Œæ‰“ä¹±

---

#### âœ… batch_size=32
**æ–‡ä»¶**: `src/ner/data.py`  
**ä»£ç è¡Œ**: ç¬¬ 91 è¡Œ

```python
def get_kfold_loaders(..., batch_size: int = 32, ...):
```

---

#### âœ… IOB æ ‡æ³¨æ ¼å¼æ”¯æŒ
**æ–‡ä»¶**: `src/ner/evaluate.py`  
**ä»£ç è¡Œ**: ç¬¬ 24-28 è¡Œ

```python
# è¿‡æ»¤å‡ºå®ä½“æ ‡ç­¾ï¼ˆæ’é™¤'O'æ ‡ç­¾ï¼‰
entity_tags = [t for t in target_names if t != 'O']
entity_indices = [tag2idx[t] for t in entity_tags if t in tag2idx]
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… æ”¯æŒ IOB æ ¼å¼ï¼šB-PER, I-PER, B-LOC, I-LOC, O ç­‰
- âœ… è‡ªåŠ¨è¯†åˆ«å’Œè¯„ä¼°å®ä½“æ ‡ç­¾

---

### 3ï¸âƒ£ è®­ç»ƒä¼˜åŒ–

#### âœ… 5-Fold äº¤å‰éªŒè¯
**æ–‡ä»¶**: `src/ner/data.py`  
**ä»£ç è¡Œ**: ç¬¬ 91-103 è¡Œ

```python
def get_kfold_loaders(..., k: int = 5, ...):
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    fold_data = []
    for train_index, val_index in kf.split(sentences):
        # ç”Ÿæˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
        # ...
    return fold_data
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… K=5ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰
- âœ… `shuffle=True` - éšæœºæ‰“ä¹±æ•°æ®
- âœ… `random_state=42` - å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°

---

#### âœ… Adam ä¼˜åŒ–å™¨ï¼ˆlr=1e-3ï¼‰
**æ–‡ä»¶**: `src/ner/train.py`  
**ä»£ç è¡Œ**: ç¬¬ 9 è¡Œ

```python
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… Adamï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–å™¨ï¼‰
- âœ… å­¦ä¹ ç‡ = 0.001

---

#### âœ… äº¤å‰ç†µæŸå¤±å‡½æ•°
**æ–‡ä»¶**: `src/ner/train.py`  
**ä»£ç è¡Œ**: ç¬¬ 8 è¡Œ

```python
criterion = nn.CrossEntropyLoss(ignore_index=tag_pad_idx)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… `CrossEntropyLoss` - å¤šåˆ†ç±»æŸå¤±å‡½æ•°
- âœ… `ignore_index=tag_pad_idx` - å¿½ç•¥å¡«å……ä½ç½®çš„æŸå¤±è®¡ç®—

---

#### âœ… è®­ç»ƒ 30 epoch
**æ–‡ä»¶**: `cli.py`  
**ä»£ç è¡Œ**: ç¬¬ 14 è¡Œ

```python
parser.add_argument('--epochs', type=int, default=3)
```

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
python cli.py --data ner_dataset.txt --model transformer --epochs 30 --folds 5
```

**æ³¨æ„**ï¼šä»£ç é»˜è®¤æ˜¯3ä¸ªepochï¼Œéœ€è¦é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®š30

---

#### âœ… å®æ—¶è¿½è¸ª 4 é¡¹æŒ‡æ ‡
**æ–‡ä»¶**: `src/ner/train.py`  
**ä»£ç è¡Œ**: ç¬¬ 10-11ã€28-33 è¡Œ

```python
train_losses, val_losses = [], []
train_accs, val_accs = [], []

# æ¯ä¸ªepochåæ‰“å°
print(f"Epoch {epoch+1}: Train Loss {total_loss:.3f}, Acc {train_accs[-1]:.3f}, Val Acc {val_acc:.3f}")
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… è®­ç»ƒæŸå¤±ï¼ˆTrain Lossï¼‰
- âœ… éªŒè¯æŸå¤±ï¼ˆVal Lossï¼‰
- âœ… è®­ç»ƒå‡†ç¡®ç‡ï¼ˆTrain Accï¼‰
- âœ… éªŒè¯å‡†ç¡®ç‡ï¼ˆVal Accï¼‰

---

#### âœ… è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ›²çº¿å¯è§†åŒ–
**æ–‡ä»¶**: `src/ner/train.py`  
**ä»£ç è¡Œ**: ç¬¬ 35-48 è¡Œ

```python
# æŸå¤±æ›²çº¿
plt.figure()
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.legend()
plt.title('Loss over Epochs')
plt.show()

# å‡†ç¡®ç‡æ›²çº¿
plt.figure()
plt.plot(train_accs, label='Train Acc')
plt.plot(val_accs, label='Val Acc')
plt.legend()
plt.title('Accuracy over Epochs')
plt.show()
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… ä½¿ç”¨ Matplotlib ç»˜å›¾
- âœ… åŒæ›²çº¿å¯¹æ¯”è®­ç»ƒé›†å’ŒéªŒè¯é›†
- âœ… ç”¨äºç›‘æ§è¿‡æ‹Ÿåˆ

---

### 4ï¸âƒ£ è¯„ä¼°åˆ†æ

#### âœ… ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1-Scoreï¼ˆMacroï¼‰
**æ–‡ä»¶**: `src/ner/evaluate.py`  
**ä»£ç è¡Œ**: ç¬¬ 4ã€27-28 è¡Œ

```python
from sklearn.metrics import classification_report, confusion_matrix

report = classification_report(all_labels, all_preds, labels=entity_indices, 
                               target_names=entity_tags, zero_division=0)
print(report)
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
              precision    recall  f1-score   support
     B-PER       0.65      0.58      0.61       245
     I-PER       0.72      0.68      0.70       198
     B-LOC       0.58      0.52      0.55       187
     ...
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… Precisionï¼ˆç²¾ç¡®ç‡ï¼‰
- âœ… Recallï¼ˆå¬å›ç‡ï¼‰
- âœ… F1-Scoreï¼ˆè°ƒå’Œå¹³å‡ï¼‰
- âœ… Macroå¹³å‡ï¼ˆæ¯ç±»æƒé‡ç›¸åŒï¼‰

---

#### âœ… æ··æ·†çŸ©é˜µ
**æ–‡ä»¶**: `src/ner/evaluate.py`  
**ä»£ç è¡Œ**: ç¬¬ 30 è¡Œ

```python
cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(idx2tag))))
```

---

#### âœ… é’ˆå¯¹æ¯ä¸ªå®ä½“ç±»åˆ«å•ç‹¬åˆ†æ
**æ–‡ä»¶**: `src/ner/evaluate.py`  
**ä»£ç è¡Œ**: ç¬¬ 24-28 è¡Œ

```python
entity_tags = [t for t in target_names if t != 'O']
entity_indices = [tag2idx[t] for t in entity_tags if t in tag2idx]
if entity_indices:
    report = classification_report(all_labels, all_preds, labels=entity_indices, 
                                   target_names=entity_tags, zero_division=0)
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… è¿‡æ»¤æ‰ 'O' æ ‡ç­¾ï¼ˆéå®ä½“ï¼‰
- âœ… åªè¯„ä¼°å®ä½“æ ‡ç­¾ï¼ˆPERã€LOCã€ORGç­‰ï¼‰
- âœ… æ¯ä¸ªç±»åˆ«å•ç‹¬è®¡ç®—æŒ‡æ ‡

---

#### âœ… ç”Ÿæˆæ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
**æ–‡ä»¶**: `src/ner/evaluate.py`  
**ä»£ç è¡Œ**: ç¬¬ 31-36 è¡Œ

```python
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', xticklabels=target_names, 
            yticklabels=target_names, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… ä½¿ç”¨ Seaborn çš„ heatmap
- âœ… `annot=True` - æ˜¾ç¤ºæ•°å€¼
- âœ… è“è‰²æ¸å˜è‰²å›¾

---

### 5ï¸âƒ£ å·¥ç¨‹å®è·µ

#### âœ… æ¨¡å—åŒ–è®¾è®¡
**é¡¹ç›®ç»“æ„**ï¼š
```
src/ner/
â”œâ”€â”€ data.py       # æ•°æ®å¤„ç†æ¨¡å—
â”œâ”€â”€ models.py     # æ¨¡å‹å®šä¹‰æ¨¡å—
â”œâ”€â”€ train.py      # è®­ç»ƒé€»è¾‘æ¨¡å—
â””â”€â”€ evaluate.py   # è¯„ä¼°åˆ†ææ¨¡å—
```

**æŠ€æœ¯ç‚¹**ï¼š
- âœ… å•ä¸€èŒè´£åŸåˆ™
- âœ… é«˜å†…èšä½è€¦åˆ
- âœ… æ˜“äºç»´æŠ¤å’Œæ‰©å±•

---

#### âœ… CLI æ¥å£
**æ–‡ä»¶**: `cli.py`  
**ä»£ç è¡Œ**: ç¬¬ 10-15 è¡Œ

```python
parser = argparse.ArgumentParser(description='NER training CLI')
parser.add_argument('--data', required=True, help='Path to dataset file')
parser.add_argument('--model', choices=['bilstm', 'transformer'], default='bilstm')
parser.add_argument('--epochs', type=int, default=3)
parser.add_argument('--folds', type=int, default=5)
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```bash
python cli.py --data ner_dataset.txt --model transformer --epochs 30 --folds 5
```

---

#### âœ… å•å…ƒæµ‹è¯•
**æ–‡ä»¶**: `tests/test_data.py`  
**ä»£ç è¡Œ**: ç¬¬ 4-12 è¡Œ

```python
def test_read_data_and_vocabs():
    sentences, tags = read_data('tests/sample_dataset.txt')
    assert len(sentences) == 2
    assert len(tags) == 2
    assert sentences[0][0] == 'John'
    w2i = build_vocab(sentences)
    t2i = build_tag_vocab(tags)
    assert '<PAD>' in w2i
    assert '<UNK>' in w2i
```

**è¿è¡Œæµ‹è¯•**ï¼š
```bash
pytest tests/test_data.py -v
```

---

#### âœ… ç¼–å†™è¯¦ç»†æŠ€æœ¯æ–‡æ¡£
**æ–‡ä»¶**: `README.md`

åŒ…å«ï¼š
- é¡¹ç›®ä»‹ç»
- æŠ€æœ¯æ ˆè¯´æ˜
- é¡¹ç›®ç»“æ„
- å¿«é€Ÿå¼€å§‹æŒ‡å—
- æ¨¡å‹æ¶æ„è¯¦è§£
- ä½¿ç”¨ç¤ºä¾‹

---

#### âœ… æä¾›çµæ´»çš„å‚æ•°é…ç½®
**CLIå‚æ•°**ï¼š
- `--data` - æ•°æ®é›†è·¯å¾„
- `--model` - æ¨¡å‹é€‰æ‹©ï¼ˆbilstm/transformerï¼‰
- `--epochs` - è®­ç»ƒè½®æ•°
- `--folds` - äº¤å‰éªŒè¯æŠ˜æ•°

**ä»£ç å†…å‚æ•°**ï¼š
- `embedding_dim`, `hidden_dim` - å¯åœ¨ `models.py` ä¿®æ”¹
- `batch_size`, `max_len` - å¯åœ¨ `data.py` ä¿®æ”¹
- `lr` - å¯åœ¨ `train.py` ä¿®æ”¹

---

## ğŸ¯ é¢è¯•å‡†å¤‡å»ºè®®

### 1. **æŠ€æœ¯æ·±åº¦é—®é¢˜å‡†å¤‡**

**Q: ä½ è¯´è‡ªå®šä¹‰å®ç°äº†ä½ç½®ç¼–ç ï¼Œå…·ä½“æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ**

**A**: "æˆ‘æ ¹æ® Transformer åŸè®ºæ–‡å®ç°äº† Sinusoidal Position Encodingã€‚å…·ä½“å…¬å¼æ˜¯ï¼š
- å¶æ•°ç»´åº¦ï¼šPE(pos, 2i) = sin(pos / 10000^(2i/d_model))
- å¥‡æ•°ç»´åº¦ï¼šPE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

ä»£ç åœ¨ `src/ner/models.py` ç¬¬22-35è¡Œã€‚ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å¥½å¤„æ˜¯å¯ä»¥è®©æ¨¡å‹å­¦ä¹ ç›¸å¯¹ä½ç½®å…³ç³»ï¼Œå¹¶ä¸”å¯¹ä»»æ„é•¿åº¦çš„åºåˆ—éƒ½æœ‰æ•ˆã€‚"

---

**Q: ä¸ºä»€ä¹ˆ BiLSTM çš„ hidden_dim æ˜¯ 256ï¼Œä½† LSTM å†…éƒ¨æ˜¯ hidden_dim // 2ï¼Ÿ**

**A**: "å› ä¸ºæ˜¯åŒå‘LSTMï¼ˆbidirectional=Trueï¼‰ã€‚å‰å‘LSTMè¾“å‡º128ç»´ï¼Œåå‘LSTMè¾“å‡º128ç»´ï¼Œæ‹¼æ¥åæ˜¯256ç»´ã€‚è¿™æ ·å¯ä»¥åŒæ—¶æ•è·ä»å·¦åˆ°å³å’Œä»å³åˆ°å·¦çš„åºåˆ—ä¿¡æ¯ã€‚ä»£ç åœ¨ `src/ner/models.py` ç¬¬11è¡Œã€‚"

---

**Q: 5-Fold äº¤å‰éªŒè¯çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ**

**A**: "åœ¨æ•°æ®é‡æœ‰é™ï¼ˆçº¦1700å¥ï¼‰çš„æƒ…å†µä¸‹ï¼Œå•æ¬¡åˆ’åˆ†å¯èƒ½æœ‰å¶ç„¶æ€§ã€‚5-Foldå°†æ•°æ®åˆ†æˆ5ä»½ï¼Œè½®æµç”¨4ä»½è®­ç»ƒã€1ä»½éªŒè¯ï¼Œå¾—åˆ°5æ¬¡å®éªŒç»“æœï¼Œæ›´èƒ½åæ˜ æ¨¡å‹çš„çœŸå®æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç åœ¨ `src/ner/data.py` ç¬¬91-103è¡Œï¼Œä½¿ç”¨äº† sklearn çš„ KFoldã€‚"

---

**Q: äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸­çš„ ignore_index æ˜¯ä»€ä¹ˆä½œç”¨ï¼Ÿ**

**A**: "ignore_index=tag_pad_idx çš„ä½œç”¨æ˜¯åœ¨è®¡ç®—æŸå¤±æ—¶å¿½ç•¥å¡«å……ä½ç½®ã€‚å› ä¸ºæˆ‘ä»¬å¯¹åºåˆ—è¿›è¡Œäº†paddingï¼Œå¡«å……çš„éƒ¨åˆ†ä¸æ˜¯çœŸå®æ•°æ®ï¼Œä¸åº”è¯¥å‚ä¸æŸå¤±è®¡ç®—å’Œæ¢¯åº¦æ›´æ–°ã€‚ä»£ç åœ¨ `src/ner/train.py` ç¬¬8è¡Œã€‚"

---

### 2. **å·¥ç¨‹èƒ½åŠ›é—®é¢˜å‡†å¤‡**

**Q: ä½ çš„ä»£ç æ˜¯å¦‚ä½•å®ç°æ¨¡å—åŒ–çš„ï¼Ÿ**

**A**: "æˆ‘å°†é¡¹ç›®åˆ†ä¸º4ä¸ªç‹¬ç«‹æ¨¡å—ï¼š
- `data.py` - æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- `models.py` - æ¨¡å‹å®šä¹‰
- `train.py` - è®­ç»ƒé€»è¾‘
- `evaluate.py` - è¯„ä¼°åˆ†æ

æ¯ä¸ªæ¨¡å—èŒè´£å•ä¸€ï¼Œé€šè¿‡ `cli.py` æ•´åˆã€‚è¿™æ ·æ˜“äºç»´æŠ¤ã€æµ‹è¯•å’Œæ‰©å±•ã€‚"

---

**Q: å¦‚ä½•ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§ï¼Ÿ**

**A**: "æˆ‘é‡‡å–äº†ä»¥ä¸‹æªæ–½ï¼š
1. å›ºå®šéšæœºç§å­ï¼šKFold ä¸­ random_state=42
2. è¯¦ç»†è®°å½•è¶…å‚æ•°ï¼šembedding_dim=128, hidden_dim=256 ç­‰
3. ç‰ˆæœ¬æ§åˆ¶ï¼šä½¿ç”¨ Git ç®¡ç†ä»£ç 
4. ç¯å¢ƒç®¡ç†ï¼šrequirements.txt é”å®šä¾èµ–ç‰ˆæœ¬"

---

### 3. **é¡¹ç›®æ”¹è¿›é—®é¢˜å‡†å¤‡**

**Q: F1 åˆ†æ•°åªæœ‰ 42%ï¼Œå¦‚ä½•æ”¹è¿›ï¼Ÿ**

**A**: "æˆ‘åˆ†æäº†å‡ ä¸ªæ”¹è¿›æ–¹å‘ï¼š
1. ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼ˆGloVe 300ç»´ï¼‰æ›¿ä»£éšæœºåˆå§‹åŒ–
2. æ·»åŠ  CRF å±‚å»ºæ¨¡æ ‡ç­¾åºåˆ—ä¾èµ–ï¼ˆBiLSTM-CRFï¼‰
3. å®ç° Early Stopping é¿å…è¿‡æ‹Ÿåˆ
4. å­¦ä¹ ç‡è¡°å‡å’Œè¶…å‚æ•°è°ƒä¼˜
5. æ•°æ®å¢å¼ºï¼ˆåŒä¹‰è¯æ›¿æ¢ã€å®ä½“æ›¿æ¢ï¼‰

è¿™äº›æ”¹è¿›é¢„è®¡å¯ä»¥å°† F1 æå‡åˆ° 65-70%ã€‚è¯¦è§ `IMPROVEMENTS.md`ã€‚"

---

## ğŸ“š ç›¸å…³æ–‡ä»¶

- **é¡¹ç›®æ–‡æ¡£**: `README.md`
- **æ”¹è¿›å»ºè®®**: `IMPROVEMENTS.md`
- **æŠ€æœ¯æ ˆ**: `requirements.txt`
- **æµ‹è¯•æ–‡ä»¶**: `tests/test_data.py`

---

## âœ… éªŒè¯æ¸…å•

åœ¨é¢è¯•å‰ï¼Œå»ºè®®è¿è¡Œä»¥ä¸‹å‘½ä»¤ç¡®è®¤ï¼š

```bash
# 1. ç»Ÿè®¡æ•°æ®é›†å¤§å°
python -c "from src.ner.data import read_data; s, t = read_data('ner_dataset.txt'); print(f'Sentences: {len(s)}')"

# 2. è¿è¡Œå•å…ƒæµ‹è¯•
pytest tests/test_data.py -v

# 3. è®­ç»ƒæ¨¡å‹å¹¶è®°å½•F1åˆ†æ•°
python cli.py --data ner_dataset.txt --model transformer --epochs 30 --folds 5

# 4. æ£€æŸ¥ä»£ç é£æ ¼
# ï¼ˆå¯é€‰ï¼‰ä½¿ç”¨ black æˆ– flake8
```

---

**æ–‡æ¡£åˆ›å»ºæ—¥æœŸ**: 2025-11-19  
**é¡¹ç›®**: Named-Entity-Recognition  
**ä½œè€…**: Jamie0807
